<!DOCTYPE html>
<html lang="zh-tw" class="no-js">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>京东零售大模型推理优化实践 - NeoTrendHub 會議報告</title>
    <meta name="description" content="AI 驅動的會議報告分析平台">
    <meta name="generator" content="Hugo 0.147.9 - NeoTrendHub">
    <meta name="author" content="NeoTrendHub AI">

    
    <meta property="og:type" content="article">
    <meta property="og:url" content="http://localhost/sessions/session-e9c5f110/">
    <meta property="og:title" content="京东零售大模型推理优化实践 - NeoTrendHub 會議報告">
    <meta property="og:description" content="AI 驅動的會議報告分析平台">

    
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="http://localhost/sessions/session-e9c5f110/">
    <meta property="twitter:title" content="京东零售大模型推理优化实践 - NeoTrendHub 會議報告">
    <meta property="twitter:description" content="AI 驅動的會議報告分析平台">

    
    <link rel="stylesheet" href="../../css/styles.css">
    <link rel="preload" href="../../css/styles.css" as="style">

    
    <link rel="canonical" href="http://localhost/sessions/session-e9c5f110/">
    

    
    <link rel="icon" type="image/x-icon" href="../../favicon.ico">

    
    <meta name="theme-color" content="#3a86ff">
    <meta name="msapplication-TileColor" content="#3a86ff">
</head>
<body class="template-professional hugo-site">
    <div class="container">
        <header class="site-header">
    <nav class="navbar">
        <div class="nav-brand">
            <a href="../../">NeoTrendHub 會議報告</a>
        </div>
        <div class="nav-menu">
            
            <a href="../../" class="nav-link">首頁</a>
            
            <a href="../../seminars/" class="nav-link">研討會</a>
            
            <a href="../../categories/" class="nav-link">分類</a>
            
            <a href="../../tags/" class="nav-link">標籤</a>
            
        </div>
        <div class="nav-toggle">
            <span></span>
            <span></span>
            <span></span>
        </div>
    </nav>
</header>

        <main class="content" id="main-content" role="main">
            
<article class="single-report">
    <header class="report-header section-block meeting-info">
        <h1 class="report-title">京东零售大模型推理优化实践</h1>
        <div class="report-meta">
            <div class="meta-item">
                <strong>研討會：</strong> 202506 AICon Beijing
            </div>
            <div class="meta-item">
                <strong>類型：</strong> 主題演講
            </div>
            <div class="meta-item">
                <strong>日期：</strong> 2025年07月18日
            </div>
            
        </div>
        
        <div class="report-tags">
            
            <a href="../../tags/%E6%95%B8%E6%93%9A" class="tag">數據</a>
            
            <a href="../../tags/%E9%9B%B2%E7%AB%AF" class="tag">雲端</a>
            
            <a href="../../tags/ai" class="tag">AI</a>
            
            <a href="../../tags/devops" class="tag">DevOps</a>
            
            <a href="../../tags/202506-aicon-beijing" class="tag">202506 AICon Beijing</a>
            
            <a href="../../tags/%E4%B8%BB%E9%A1%8C%E6%BC%94%E8%AC%9B" class="tag">主題演講</a>
            
        </div>
        
    </header>

    <div class="report-content">
        <h2 id="會議資訊">會議資訊</h2>
<ul>
<li><strong>研討會：</strong> 202506 AICon Beijing</li>
<li><strong>類型：</strong> 主題演講</li>
<li><strong>來源：</strong> <a href="https://aicon.infoq.cn/2025/beijing/presentation/6530">https://aicon.infoq.cn/2025/beijing/presentation/6530</a></li>
</ul>
<hr>
<h2 id="報告內容">報告內容</h2>
<p>好的，這是一份根據您提供的京東零售大模型推理優化實踐 PPT 內容，生成的全方位綜合分析報告。</p>
<hr>
<h2 id="京東零售大模型推理優化實踐aicon-2025-全方位綜合分析報告">京東零售大模型推理優化實踐：AICon 2025 全方位綜合分析報告</h2>
<h3 id="報告摘要">報告摘要</h3>
<p>本報告基於京東零售於 AiCon Beijing 2025 全球人工智能開發與應用大會上發表的「京東零售大模型推理優化實踐」主題演講內容，深入剖析了京東在大模型推理領域的先進技術、商業應用與未來戰略。京東作為電商巨頭，在大模型時代面臨著極致性能與成本效率的雙重挑戰。本次演講詳盡展示了其在系統架構、模型層面、硬體協同等多維度進行的創新優化，不僅在技術上取得突破，更在業務應用中實現了顯著的效益提升，為大模型在複雜零售場景中的大規模落地提供了寶貴經驗。</p>
<hr>
<h3 id="1-會議概述和核心內容">1. 會議概述和核心內容</h3>
<p>本次主題演講由京東 AI 架構師楊培軍先生在 AiCon Beijing 2025 大會上發表，聚焦於京東零售在大模型推理優化方面的深度實踐。演講旨在分享京東如何應對大型語言模型（LLM）及多模態大模型在實際電商業務場景中部署所面臨的性能、成本與穩定性挑戰，並揭示其所採取的獨特技術方案。</p>
<p>演講內容脈絡清晰，主要圍繞以下四大核心板塊展開：</p>
<ul>
<li><strong>大模型應用場景：</strong> 闡述大模型在電商領域的廣闊應用前景，涵蓋生成式 AI、智能體 AI 及實體 AI 等多個維度。</li>
<li><strong>大模型應用挑戰：</strong> 深入分析大模型推理所面臨的硬體資源限制、模型特性與服務場景多樣性帶來的諸多技術難題。</li>
<li><strong>核心優化實踐：</strong> 詳細介紹京東為解決這些挑戰所採取的具體技術方案，包括分離式架構、KV Cache 優化、多級負載均衡、多層流水線 Overlap 等。</li>
<li><strong>總結及展望：</strong> 歸納優化成果，並對未來大模型技術發展趨勢、軟硬協同方向及與生成式推薦等領域的融合進行前瞻性思考。</li>
</ul>
<p>整體而言，本次演講不僅是對京東技術實力的展示，更是對行業痛點的精準洞察與解決方案的系統性呈現，對推動大模型在真實商業環境中的應用具有重要指導意義。</p>
<hr>
<h3 id="2-技術要點和實現細節">2. 技術要點和實現細節</h3>
<p>京東零售在大模型推理優化方面展現了極為系統和深入的技術實力，其核心要點和實現細節涵蓋了從底層硬體、系統架構到模型運行的多個層次：</p>
<h4 id="21-大模型應用挑戰痛點分析">2.1 大模型應用挑戰：痛點分析</h4>
<ul>
<li><strong>資源瓶頸與算力利用率低下：</strong> 大模型參數爆炸式增長（如 DeepSeekR1-671B FP8 佔 671GB），遠超單卡顯存容量（H20 僅 96GB）。Transformer 的自回歸解碼過程在 Prefill（計算密集）和 Decode（訪存密集）階段特性差異巨大，導致串行執行時 GPU 算力利用率低下，特別是 Decode 階段。</li>
<li><strong>KV Cache 壓力：</strong> KV Cache 隨序列長度與 Batch Size 呈線性增長，極大地加劇了顯存容量和訪存帶寬的壓力，成為「内存牆」的核心因素。例如 LLaMA2-13B 在 4K 長序列下 KV Cache 可達 3GB。</li>
<li><strong>服務場景多樣性與複雜性：</strong> 面臨多變的用戶請求（流量高峰、輸入/輸出長度變化）、不同的請求優先級和 SLO（服務等級目標）要求、多模態數據支持，以及異構硬體和分散式調度帶來的挑戰。</li>
</ul>
<h4 id="22-核心優化實踐系統級模型級與硬體級協同">2.2 核心優化實踐：系統級、模型級與硬體級協同</h4>
<p>京東的推理優化圍繞高吞吐（Tokens/秒）、低時延（TTFT、TBT）和高可用性展開，採用了多維度、分層次的綜合策略：</p>
<ul>
<li>
<p><strong>分離式架構 (Separated Architecture)：</strong></p>
<ul>
<li><strong>理念：</strong> 深度解耦傳統單體模式，將模型結構、計算階段和存儲資源分離，以構建彈性、高效、可擴展的分布式推理系統。</li>
<li><strong>多維混合并行：</strong>
<ul>
<li><strong>Tensor Parallel (TP)：</strong> 將模型張量切分到多設備上，降低單設備内存壓力並提高計算并行度，但通信量大。</li>
<li><strong>Pipeline Parallel (PP)：</strong> 將模型層切分到不同設備上，適合超深模型，但可能產生流水線氣泡 (bubble)。</li>
<li><strong>Expert Parallel (EP)：</strong> 針對 MoE 模型，將不同專家模型分散部署，降低内存壓力，但需處理專家之間的通信開銷。</li>
<li><strong>Data Parallel (DP)：</strong> 在多設備上複製模型，處理不同數據批次，避免重複計算和存儲。</li>
<li><strong>MoE 混合并行實例：</strong> 在 16 個 GPU 上實現 EP/TP/DP 混合并行，針對 MoE 模型進行專家切片 (Expert-slicing) 和張量切片 (Tensor-slicing) 的協同工作。</li>
</ul>
</li>
<li><strong>Prefill-Decode (PD) 分離：</strong>
<ul>
<li><strong>動機：</strong> 解決連續批處理 (Continuous Batching) 中 Prefill（計算密集）和 Decode（訪存密集）階段資源需求和計算特性差異大導致的相互干擾和資源浪費問題。</li>
<li><strong>實現：</strong> 將 Prefill 和 Decode 資源隔離部署，各自獨立調度與優化，如為兩者定制不同的資源卡型和并行策略，從而有效降低 TTFT 和 TBT。</li>
<li><strong>KV Cache 傳輸優化：</strong> 針對 PD 分離中 KV Cache 的傳輸，從 PD Device 直推或 Offload DRAM 兩種模式選擇，並將 KV Cache 傳輸進行分塊（Layer-wise 或按需合併），以適應不同序列長度，減少傳輸時延。</li>
</ul>
</li>
<li><strong>Encoder-Prefill-Decode (EPD) 分離：</strong> 針對多模態大模型（VLM），進一步將圖像預處理、圖像編碼（Encoder）與文本解碼（Decode）階段分離，實現資源隔離和異步并行，提升 VLM 推理效率。</li>
<li><strong>KV Cache Pool：</strong>
<ul>
<li><strong>動機：</strong> 解決 KV Cache 對 HBM（高頻寬記憶體）的巨大佔用問題。</li>
<li><strong>實現：</strong> 將 KV Cache Offload 到 DRAM/SSD 以擴展存儲空間，並將不同 Instance 的 HBM/DRAM/SSD 多級 KV Cache 存儲進行全局共享管理，形成「KV Cache Pool」，用於全局 Prefix Cache Pool 和 PD KV Cache 傳輸 Pool，極大提升内存利用率和緩存命中率。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>多級負載均衡：</strong></p>
<ul>
<li><strong>動機：</strong> 傳統 RoundRobin 無法感知 Instance 實時負載（特別是 KV Cache 狀態），導致匹配率低和熱點問題。</li>
<li><strong>實現：</strong> 引入<strong>全局調度器</strong>，實現 Prefix Cache Aware 和 KV Cache Aware 的智能調度，平衡 Instance 負載。針對分布式并行場景，引入 <strong>DPLB (Data Parallel Load Balancing)</strong> 和 <strong>EPLB (Expert Parallel Load Balancing)</strong>，解決單子 Batch 負載差異和熱點專家負載高等問題，全面提升整體時延表現。</li>
</ul>
</li>
<li>
<p><strong>多層流水線 Overlap：</strong></p>
<ul>
<li><strong>動機：</strong> 解決 CPU 調度、GPU/NPU 計算、通信和數據搬運之間的串行執行導致的資源利用率低下和「氣泡」問題。</li>
<li><strong>實現：</strong>
<ul>
<li><strong>CPU 調度和 NPU 計算異步流水線：</strong> 使用 Fake Token 機制提前執行下一個迭代的 CPU 調度，在 NPU 計算前替換為真 Token，實現 CPU 和 NPU 的并行。</li>
<li><strong>NPU 計算和通信異步流水線：</strong> 採用雙 Micro Batch 策略，將當前 Batch 的計算與上一個 Batch 的通信重疊，充分利用通信與計算資源。</li>
<li><strong>NPU Cube/Vector 計算單元與訪存流水并行：</strong> 硬體層面的指令級并行，最大化晶片內部利用率。</li>
<li><strong>DeepSeek MTP 投機推理優化：</strong> 針對 DeepSeek 的投機解碼機制，通過提前準備下一迭代 Target Model 和 Draft Model 的輸入，減少模型間交互的等待時間，進一步提升 1.3-1.5 倍效率。</li>
<li><strong>PD 分離通信與計算 Overlap：</strong> 利用 LayerWise 異步傳輸 KV Cache，使 KV Cache 的傳輸與當前迭代的 Forward 計算重疊，顯著降低長序列傳輸耗時。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>高性能基礎設施：</strong></p>
<ul>
<li><strong>純 C++ Runtime 和高性能算子：</strong> 底層採用純 C++ 實現推理運行時，並優化了 FlashAttention、FlashInfer 等高性能算子，結合 Kernel Launch 和 CudaGraph 技術，最大化 GPU/NPU 算力利用率。</li>
<li><strong>模型壓縮與結構優化：</strong> 支持 AWQ/Smooth Quant、INT4/INT8/FP8 等量化技術，以及 MQA/GQA 等模型結構優化，減少模型大小和計算量。</li>
<li><strong>硬體支持：</strong> 不僅支持 NVIDIA GPU，也積極擁抱國產 NPU，並配合京東自研的 CloudMatrix384 超節點（全對等互聯、光通信）等先進硬體，實現軟硬深度協同。</li>
</ul>
</li>
</ul>
<p>這些技術要點共同構建了一個高度優化、彈性且高效的大模型推理系統，全面提升了京東大模型在電商複雜場景下的運行效率和成本效益。</p>
<hr>
<h3 id="3-商業價值和應用場景">3. 商業價值和應用場景</h3>
<p>京東零售的大模型推理優化不僅停留在技術層面，更在實際業務中展現了巨大的商業價值，深刻改變了電商的運營模式與用戶體驗。</p>
<h4 id="31-廣泛的應用場景">3.1 廣泛的應用場景</h4>
<p>京東將大模型技術深入應用於電商全鏈路，主要分為三大類：</p>
<ul>
<li>
<p><strong>生成式 AI (Generative AI)：</strong></p>
<ul>
<li><strong>內容自動生成：</strong> AI 商品圖生成、AI 視頻生成、AI 營銷內容生成，大幅降低內容製作成本，提升效率和個性化程度。</li>
<li><strong>數字人應用：</strong> AI 模特圖生成、AI 數字人直播（適用於 3C、家居、媒體等場景），提供更沉浸、互動性強的購物體驗。</li>
</ul>
</li>
<li>
<p><strong>智能體 AI (Agentic AI)：</strong></p>
<ul>
<li><strong>智能客服與售後管理：</strong> 自動化處理大量諮詢、退換貨等，提升客戶服務效率和滿意度。</li>
<li><strong>AI 經營托管：</strong> 輔助商家進行經營決策，如銷量預測、採購建議等。</li>
<li><strong>AI 倉配優化：</strong> 精準預測物流需求，優化倉儲布局和配送路徑，提高效率，降低成本。</li>
<li><strong>AI 交互式推薦：</strong> 提供可交互導購、商品對比、商品總結、購物建議等，實現千人千面的個性化推薦，提升用戶購物體驗和轉化率。</li>
</ul>
</li>
<li>
<p><strong>實體 AI (Physical AI)：</strong></p>
<ul>
<li><strong>自動分揀機器人：</strong> 提升倉儲物流的自動化水平和效率。</li>
<li><strong>智能空間：</strong> 應用於實體店鋪或智慧倉儲。</li>
<li><strong>自動駕駛：</strong> 探索物流運輸的未來模式。</li>
</ul>
</li>
</ul>
<h4 id="32-顯著的業務落地效果">3.2 顯著的業務落地效果</h4>
<p>京東的推理優化實踐在核心業務場景中取得了令人矚目的量化成果：</p>
<ul>
<li>
<p><strong>可交互導購場景：</strong></p>
<ul>
<li><strong>時延大幅降低：</strong> TP99 時延（即 99% 的請求延遲低於此值）下降 <strong>50%</strong>，意味著絕大多數用戶能體驗到更快的響應速度，極大提升了互動流暢性。</li>
<li><strong>資源效率顯著提升：</strong> 資源節省約 <strong>60%</strong>，直接降低了運營成本，實現了更高效的資源利用。</li>
<li><strong>用戶體驗與轉化率提升：</strong> UCVR（User Click-Through Value Rate，用戶點擊價值率）提升 <strong>+5%</strong>，活躍用戶占比提升 <strong>+2%</strong>，直接證明了低時延和智能服務對用戶行為的積極影響。</li>
</ul>
</li>
<li>
<p><strong>商品理解場景：</strong></p>
<ul>
<li><strong>大模型吞吐能力飛躍：</strong> 大模型整體吞吐量提升 <strong>3 倍</strong>，意味著單位時間內能處理更多商品信息。</li>
<li><strong>推理成本大幅削減：</strong> 模型推理成本節省約 <strong>70%</strong>，為大規模模型部署提供了經濟可行性。</li>
<li><strong>多模態處理能力爆發：</strong> 多模態大模型推理吞吐提升約 <strong>20 倍</strong>，這對於京東龐大的商品圖片、視頻和文字數據處理至關重要。</li>
<li><strong>標簽實效性顯著提升：</strong> 支持商品標簽的實效性提升 <strong>10 倍以上</strong>，確保商品信息更新及時、準確，進而影響推薦、搜索和營銷效果。</li>
</ul>
</li>
</ul>
<p>這些數據充分證明了京東在推理優化上的投入已轉化為實實在在的商業價值，不僅提升了用戶體驗和經營效率，更大幅降低了大規模 AI 應用的門檻和成本。</p>
<hr>
<h3 id="4-創新亮點和技術突破">4. 創新亮點和技術突破</h3>
<p>京東零售在大模型推理優化方面的實踐，不僅僅是技術的應用，更包含了一系列針對大模型特性和電商場景的深度創新和突破：</p>
<ul>
<li>
<p><strong>分層次、多維度的立體優化策略：</strong> 京東沒有停留在單一維度的優化（如僅模型量化或算子優化），而是從系統架構、計算模式、數據管理到軟硬協同，進行了全面而深入的優化。這種系統性的思考和實現，是其取得顯著成果的關鍵。</p>
</li>
<li>
<p><strong>「分離式架構」的創新型設計：</strong> 深度解耦模型結構、計算階段（Prefill/Decode/Encoder）和存儲資源，通過 PD/EPD 分離和 KV Cache Pool 的設計，突破了單機算力/顯存瓶頸，並解決了不同計算階段資源需求錯配的問題。這是一種對傳統單體推理服務模式的顛覆性創新。</p>
</li>
<li>
<p><strong>智能與精細化的負載均衡與調度：</strong> 引入全局調度器，結合 Prefix Cache 和 KV Cache 狀態進行<strong>實時、感知型</strong>的調度，以及針對 DP 和 EP 並行場景的 DPLB 和 EPLB，極大地提升了資源的匹配率和整體集群效率，有效避免了熱點問題。這比傳統的簡單負載均衡機制要複雜且智能得多。</p>
</li>
<li>
<p><strong>極致的「多層流水線 Overlap」：</strong> 針對大模型推理過程中存在的 CPU/NPU 異步、計算/通信串行、算子內部流水線氣泡等問題，京東設計了多層次的異步流水線機制，從宏觀的 CPU 調度到微觀的 NPU 單元級別，實現了通信與計算的深度重疊，最大化了硬體資源利用率，尤其對 DeepSeek MTP 投機推理的優化，展現了對前沿模型的深度適配能力。</p>
</li>
<li>
<p><strong>軟硬深度協同的實踐：</strong> 京東不僅在軟體層面進行優化，更強調與硬體的深度協同，例如其對國產 NPU 的支持，以及自身在 CloudMatrix384 等超節點硬體上的探索。這種從晶片設計到應用層的垂直整合思維，是實現極致性能的保障。</p>
</li>
<li>
<p><strong>業務場景痛點驅動的技術創新：</strong> 京東的優化不是為技術而技術，而是緊密圍繞電商核心業務場景（如交互式導購、商品理解）的痛點（高時延、高成本）展開。最終實現的時延降低、成本節省、吞吐提升等效果，直接轉化為商業指標的顯著改善，證明了其技術創新與商業價值的緊密結合。</p>
</li>
</ul>
<p>這些創新亮點共同構成京東在大模型推理優化領域的領先地位，不僅解決了當前的技術難題，也為未來大規模 AI 應用提供了堅實的技術基石。</p>
<hr>
<h3 id="5-趨勢洞察和未來展望">5. 趨勢洞察和未來展望</h3>
<p>京東在演講中不僅回顧了過去的成就，更對大模型的未來發展趨勢及自身戰略方向進行了深度洞察與展望。</p>
<h4 id="51-行業趨勢洞察">5.1 行業趨勢洞察</h4>
<ul>
<li><strong>頭部玩家全面入局與技術深化：</strong> 互聯網巨頭正全面投入大模型競賽，推動技術快速迭代。大模型本身也在多模態感知、AI Agent 和邊緣智能部署等維度持續取得突破。</li>
<li><strong>大模型驅動電商全鏈路變革：</strong> 大模型不僅是技術工具，更是重構電商技術基座和業務流程的核心驅動力，從內容生產到用戶交互再到供應鏈管理，都將被深刻影響。</li>
<li><strong>開源生態的崛起與成本顛覆：</strong> DeepSeek 等開源大模型展現了與頂級閉源模型媲美的性能，並在訓練和推理成本上實現數量級的降低，這預示著大模型技術的普及化和應用門檻的降低，將加速產業落地。</li>
</ul>
<h4 id="52-京東的未來展望">5.2 京東的未來展望</h4>
<p>京東的未來展望體現了其對大模型技術發展的深刻理解和務實的落地策略，主要從系統、模型和硬體三個層面展開：</p>
<ul>
<li>
<p><strong>系統層面：</strong></p>
<ul>
<li><strong>分布式架構的持續賦能：</strong> 京東將繼續借鑒傳統分布式系統的豐富經驗，進一步強化其分布式架構在高性能、強擴展性和高可用性方面的能力。</li>
<li><strong>智能集群調度：</strong> 發展更智能的集群調度能力，如 AutoPD（自動 Prefill-Decode 分離）和 Dynamic EP（動態專家并行），以實現資源的動態優化配置。</li>
<li><strong>高效容錯機制：</strong> 構建更高效的 Failover（故障轉移）、KV Cache 重算及遷移備份等能力，確保系統在高負載和故障情況下的穩定性與可用性。</li>
</ul>
</li>
<li>
<p><strong>模型層面：</strong></p>
<ul>
<li><strong>深度理解模型結構與性能挖掘：</strong> 繼續深入理解不同模型（如 Transformer、MoE）的內部結構和計算特性，挖掘硬體的理論性能極限。</li>
<li><strong>高效算子與并行優化：</strong> 持續開發和優化 Fusion Kernel（算子融合）以及各種并行技術（如 FlashAttention、FlashMLA、LargeEP），並將訪存、通信與計算的 Overlap 推向極致，以最大化硬體資源利用率。</li>
</ul>
</li>
<li>
<p><strong>硬體層面：</strong></p>
<ul>
<li><strong>軟硬深度協同：</strong> 強調軟體與硬體之間的深度協同設計，這被視為打開算力、存儲和通信天花板的關鍵。</li>
<li><strong>先進硬體基礎設施：</strong> 以京東自研的 CloudMatrix384 超節點為代表，通過全對等互聯架構和光通信技術，旨在實現單集群如「一體機」般的極致性能和便捷管理。</li>
</ul>
</li>
<li>
<p><strong>技術同源性與跨領域應用：</strong></p>
<ul>
<li><strong>生成式推薦與 LLM 技術的融合：</strong> 京東敏銳地洞察到生成式推薦（如 Kuaishou OneRec, Meta GR）和生成式檢索（如 Google TIGER）與 LLM 推理技術的深層同源性。許多 LLM 推理優化技術，如序列緩存（類似 Prefix Cache）、分離式架構（類似 EPD 分離）、存儲與計算解耦（類似 KV Cache Pool）以及長序列建模（類似 SIM GSU 序列壓縮）等，可以直接或借鑒應用於推薦與檢索場景。</li>
<li><strong>挑戰與機遇並存：</strong> 儘管存在同源性，但在推薦場景中，仍面臨大規模稀疏特征表達（億級 vs LLM 萬級 Vocabulary）、高流量以及「完全 list-wise 自回歸生成」帶來的巨大資源消耗等挑戰。京東將持續探索 Tokenizer/DeTokenizer、Vocabulary 規模和採樣策略，以平衡效果與性能。</li>
</ul>
</li>
</ul>
<p>總之，京東的未來展望展現了其在 AI 領域的戰略視野，不僅專注於大模型核心技術的深耕，更放眼於如何將這些技術擴展至更廣泛的商業應用場景，並通過軟硬體協同和生態合作，持續引領電商 AI 的發展。</p>
<hr>
<hr>
<div style="text-align: center; color: #666; font-size: 0.9em; margin-top: 2em;">
<em>本報告由 NeoTrendHub 自動生成 | 生成時間：2025-07-18 14:12:13</em>
</div>

    </div>

    <footer class="report-footer section-block">
        <div class="report-navigation">
            
            <a href="http://localhost/sessions/session-dfa27999/" class="nav-link prev">
                <span class="nav-label">上一篇</span>
                <span class="nav-title">人机协同新范式：AI&#43;AR 空间计算的落地路径</span>
            </a>
            

            
            <a href="http://localhost/sessions/session-188a161a/" class="nav-link next">
                <span class="nav-label">下一篇</span>
                <span class="nav-title">一年上线超 10 款产品，AI 时代如何做独立开发</span>
            </a>
            
        </div>

        <div class="report-info">
            <p><em>本報告由 NeoTrendHub 自動生成 | 生成時間：2025-07-18 00:00:00</em></p>
        </div>
    </footer>
</article>

        </main>

        <footer class="site-footer">
    <div class="footer-content">
        <div class="footer-info">
            <p>&copy; 2025 NeoTrendHub 會議報告. 由 NeoTrendHub 驅動。</p>
            <p>AI 驅動的會議報告分析平台</p>
        </div>
        <div class="footer-links">
            <a href="../../">首頁</a>
            <a href="../../seminars/">研討會</a>
            <a href="../../categories/">分類</a>
            <a href="../../tags/">標籤</a>
        </div>
    </div>
</footer>
    </div>

    
    <script src="../../js/main.js" defer></script>

    
    <script>document.documentElement.classList.remove('no-js');</script>
</body>
</html>