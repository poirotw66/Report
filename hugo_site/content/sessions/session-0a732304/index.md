---
analysis_mode: comprehensive
category: 主題演講
date: '2025-07-18'
draft: false
layout: single
seminar: 202506 AICon Beijing
session_id: 0a732304-f12c-4f31-895a-8d165e0a0e5d_Training_Inference_Agents_Beyond_Apps_in_the_AI-Na
tags:
- 數據
- 雲端
- AI
- DevOps
- 後端
- 安全
- 微服務
- 202506 AICon Beijing
- 主題演講
template_style: professional
title: 'Training, Inference, Agents: Beyond Apps in the AI-Native World'
type: posts
---

## 會議資訊
- **研討會：** 202506 AICon Beijing
- **類型：** 主題演講
- **來源：** [https://aicon.infoq.cn/2025/beijing/presentation/6531](https://aicon.infoq.cn/2025/beijing/presentation/6531)

---

## 報告內容

## 「訓練、推論、代理：AI原生世界中超越應用程式的發展」綜合分析報告

### 報告概要

本報告旨在深入分析Mark Collier先生在「202506 AICon Beijing」主題演講《Training, Inference, Agents: Beyond Apps in the AI-Native World》的PPT內容。本次演講聚焦於在「AI原生」（AI-Native）新時代下，人工智能發展的三大核心支柱：訓練（Training）、推論（Inference）和代理（Agents）。報告將從會議概述、技術要點、商業價值、創新亮點以及趨勢洞察等多個維度，為讀者呈現一份全面而深入的分析，旨在闡明開源、基礎設施與社群在AI未來發展中的關鍵作用。

---

### 1. 會議概述和核心內容

**會議主題與背景：**
本次主題演講《Training, Inference, Agents: Beyond Apps in the AI-Native World》由Linux基金會AI與基礎設施總經理、OpenStack和OpenInfra基金會的共同創辦人Mark Collier先生，於2025年6月在北京的AiCon全球人工智能開發與應用大會上發表。演講的核心思想是，在一個由AI模型主導思考和互動的「AI原生」世界中，AI的發展將超越傳統應用程式的範疇，而其基石在於高效的訓練、推論以及智能代理系統。

**核心論點：**
演講者明確指出，AI的蓬勃發展對底層基礎設施提出了前所未有的巨大需求。他強調，AI與基礎設施密不可分，當前AI面臨的最大挑戰之一正是基礎設施層面的瓶頸，如GPU稀缺、高能耗、延遲問題以及數據重力（data gravity）。為了解決這些挑戰並推動AI的開放與普及，開源（Open Source）策略顯得至關重要。DeepSeek等開源模型的異軍突起，已證明了閉源系統的「護城河」是短命的，開源模式將加速創新、促進廣泛採用並增強可訪問性，將價值和創新推向技術堆疊的頂層。

基於此洞察，演講詳細闡述了AI原生計算的三大支柱：
1.  **訓練 (Training)：** 關於AI模型如何透過數據學習和優化。
2.  **推論 (Inference)：** 關於已訓練模型如何進行實時預測和應用。
3.  **代理 (Agents)：** 關於AI模型如何被賦予行動能力，並透過協議與工具實現自主決策和協作。

貫穿始終的共同主題是：開源生態、健壯的基礎設施以及活躍的社群，是確保AI原生時代開放、互通與繁榮的關鍵要素。演講最終呼籲業界共同投資並建立開放的AI未來。

---

### 2. 技術要點和實現細節

本次演講深入探討了AI原生計算的技術棧，以下是其核心技術要點和實現細節：

**2.1 AI原生計算的基礎設施要求：**
*   **數據量與處理能力：** AI對數據的傳輸、儲存與管理提出海量需求，如Google每月處理480兆tokens，比一年前增長50倍。這直接導致對計算能力和儲存帶寬的巨大壓力。
*   **硬體加速器與效率：** GPU稀缺、高能耗和延遲是AI基礎設施面臨的主要挑戰。推論工作負載是訓練的50倍，對效率有極高要求，需要專門的硬體或軟體優化以提高速度和效率。
*   **混合雲/多雲互操作性：** 複雜的環境下，如何確保模型和數據在不同雲平台間的順暢流動與協同作業。
*   **開放基礎設施基石：** 演講強調Linux、OpenStack和Kubernetes等開放基礎設施項目在支撐全球計算生態系統中的核心地位，尤其指出OpenStack在中國的廣泛應用（數百萬計算核心、支援支付寶等）。這些開放平台為AI的發展提供了堅實的底層支援。

**2.2 AI原生計算的三大支柱及其技術棧：**

*   **支柱 ① 訓練 (Training)：**
    *   **定義：** 輸入數據，調整演算法參數，輸出可預測的模型。
    *   **核心技術棧：**
        *   **框架/函式庫：** PyTorch佔據主導地位（80%研究人員使用），TensorFlow和JAX則逐漸被Transformers庫棄用，顯示PyTorch生態的強勢。
        *   **基礎設施黏合層：** DeepSpeed、Megatron-LM、Ray等，用於大規模分散式訓練，提高效率和可擴展性。
        *   **預訓練檢查點與數據：** DeepSeek-V3、Mistral-7B、RedPajama等開放模型和數據集，加速了模型訓練的迭代和下游創新。
        *   **模型相容性與匯出：** ONNX（Open Neural Network Exchange）用於確保不同框架訓練的模型能夠在各種運行時環境中互操作，Hugging Face Transformers則提供了標準化的模型介面和生態。
    *   **關鍵要點：** 開放權重和數據集是實現快速迭代和下游創新的關鍵。中立的基金會治理模式（如Linux基金會）在建立信任和推動生態系統發展方面發揮著重要作用。

*   **支柱 ② 推論 (Inference)：**
    *   **定義：** 載入已訓練模型，接收實時請求，返回預測結果。其可以在從邊緣晶片到資料中心的各種環境中運行。
    *   **核心技術棧：**
        *   **運行時/圖執行：** ONNX Runtime、TensorRT-LLM，負責高效執行模型圖。
        *   **分散式引擎：** vLLM已被稱為「事實上的開放生成式AI推論平台」，能夠在各種硬體和雲環境中連接多種LLM，並透過OpenStack整合。其他如abrix和LLM-d也在此領域發展。
        *   **服務框架：** KServe、Triton Inference Server，用於標準化模型服務部署在Kubernetes/OpenStack上。
        *   **優化/快取：** LM Cache等技術，提高推論效率。
    *   **核心概念：LLM作為新作業系統。** 演講借鑒Andrej Karpathy的「軟體3.0」概念，將LLM比作新一代作業系統：
        *   **核心（Kernel）** → 推論運行時（ONNX Runtime, vLLM, LLM-d），負責調度tokens。
        *   **系統呼叫（Syscalls）** → 代理協議（MCP, A2A），標準化函數呼叫、生成代理等。
        *   **使用者空間應用（User-land apps）** → 代理工作流程（LangChain, CrewAI, Autogen）。
        *   **週邊設備（Peripherals）** → 工具/技能（插件、解釋器、資料庫）。
    *   **關鍵要點：** 推論是AI產品化和商業化的核心，其效率和可靠性至關重要。LLM-d作為基於Kubernetes的大規模分散式推論項目，若成功，可能成為「AI的核心」。多加速器調度抽象是未來發展的關鍵。

*   **支柱 ③ 代理 (Agents)：**
    *   **定義：** 將模型封裝在程式碼中，使其能夠呼叫工具、與其他代理或人類對話，並透過開放協議（如MCP）將推論結果轉化為行動。
    *   **核心技術棧：**
        *   **協議與標準：** MCP（Model Communication Protocol）和A2A（Agent2Agent Protocol）是實現代理間互操作的關鍵。Google Cloud將A2A轉移到Linux基金會，並有多家科技巨頭（Amazon、Cisco、Google、Microsoft等）參與，顯示其標準化和開放治理的重要性。
        *   **代理框架：** LangChain、CrewAI、IDEA、ChatDev，提供構建代理工作流程的工具和抽象。
        *   **工具註冊中心/API：** OpenAI Function Calling、BentoML Tool Hub，使代理能夠發現和使用外部工具。
        *   **底層基礎設施：** 技能/工具插件（API管理、企業資源）、記憶體/向量儲存（Chroma, Weaviate, pgvector, Redis-Vector）是代理運行的關鍵支援。
    *   **核心概念：情境工程（Context Engineering）與代理速度。** 軟體焦點從「使用者速度」轉向「代理速度」，即不再僅限於人類鍵盤輸入的速度，而是代理池的無限吞吐量。與此同時，「情境工程」（將正確資訊填入代理上下文）的重要性遠超「提示工程」。
    *   **關鍵要點：** 代理仍處早期階段，但潛力巨大。標準協議是避免供應商鎖定和實現互通性的基石。小型語言模型（SLM）因其效率和適用性，被視為代理式AI的未來。

---

### 3. 商業價值和應用場景

AI原生世界的三大支柱不僅帶來技術上的進步，更孕育著巨大的商業價值和廣泛的應用場景。

*   **自動化與效率提升：**
    *   **軟體自行編寫：** 微軟、Google、Meta等科技巨頭已證實AI能自動生成高達30%-50%的程式碼。這將極大地提高開發效率，縮短產品上市時間，並降低開發成本。
    *   **代理自動化工作流程：** AI代理能夠自動化重複性工作（如程式碼生成、文件處理、運營操作），將人類從繁瑣任務中解放出來，專注於規劃、審查和協調等高階任務，從根本上重塑企業的營運模式。
    *   **推論效率優化：** 推論作為AI工作負載的絕大部分（50倍於訓練），其效率的提升直接關係到AI產品的商業可行性和成本效益。高效的推論平台如vLLM能顯著降低企業運行AI服務的TCO（總擁有成本），使得AI規模化應用成為可能。

*   **市場顛覆與新商機：**
    *   **開源的市場衝擊：** DeepSeek等開源模型的崛起，導致Nvidia股價下跌，並揭示了「大型語言模型之間沒有護城河」的市場現實。這意味著AI模型不再是少數巨頭的專利，中小企業和新創公司可透過開源模型快速切入市場，激發更廣泛的創新和競爭。
    *   **價值鏈的重塑：** 開源軟體（OSS）透過降低雲平台、軟體、硬體和服務的底層成本，將價值和創新推向了技術堆疊的頂層，例如基礎模型、微調專業模型、模型中心和AI安全等。這為企業在這些高附加值領域創造了新的商業機會。
    *   **AI作為服務（AIaaS）：** 標準化的推論服務框架（KServe, Triton）和代理協議（A2A）使得AI能力可以更便捷地作為服務被整合和消費，催生更多基於AI的垂直行業解決方案和創新應用。

*   **行業應用廣度：**
    *   **企業級應用：** OpenStack在中國金融（支付寶）、電信（中國移動、電信、聯通）、能源（國家電網）等關鍵基礎設施中的應用，證明了開放基礎設施在承載核心業務方面的成熟度。AI的加入將使這些傳統行業實現更深度的智能轉型。
    *   **新興AI應用：** 自動駕駛（BMW, Hyundai, Volvo）、智能硬件、具身智能、金融+大模型、多模態大模型、LMOps等，都是未來AI的重要應用場景，預計將深度整合AI原生計算的三大支柱。
    *   **軟體產品設計的轉變：** 從以人類使用者為中心的設計轉變為以AI代理為中心的設計，產品使用者體驗將更多地優化於「規劃、審查和協調」等，這將催生全新的軟體產品形態和服務模式。

---

### 4. 創新亮點和技術突破

本次演講所呈現的內容，不僅是對AI發展現狀的歸納，更揭示了多個前瞻性的創新亮點和潛在的技術突破：

*   **AI模型性能的開源超越：**
    *   最引人注目的創新之一是開源AI模型（如DeepSeek R1）在評估指標上已「大體趕上甚至超越閉源AI模型」。這顛覆了過去專有模型主導的局面，證實了開源社群的力量和快速迭代能力。DeepSeek R1在Hugging Face上數週內破千萬下載量，證明了開源模式在創新傳播和廣泛採用方面的巨大優勢。

*   **「LLM作為新作業系統」的範式轉移：**
    *   Andrej Karpathy的「軟體3.0」概念被引申為「LLM作為新作業系統」的思考，這是一個極具突破性的創新觀點。它將AI推論運行時視為系統核心，代理協議為系統呼叫，代理工作流程為使用者空間應用，工具為週邊設備。這種視角不僅重新定義了程式設計和系統互動方式，更預示著一個以模型為中心、而非傳統程式碼為中心的全新計算範式，可能從根本上改變軟體的設計、開發和部署。

*   **情境工程（Context Engineering）的崛起：**
    *   演講強調「情境工程」的重要性遠超傳統的「提示工程」。這是一項新的關鍵技能，旨在藝術與科學地將正確的資訊填充到AI模型的上下文視窗中，以實現代理的準確性和效率。這項創新直接影響到代理的性能和成本，是實現複雜、可靠AI代理的基石。

*   **AI代理標準協議的里程碑式發展：**
    *   MCP（Model Communication Protocol）和A2A（Agent2Agent Protocol）的提出與發展，特別是Google Cloud將A2A項目移交給Linux基金會，並獲得Amazon、Cisco、Microsoft等巨頭的共同參與，標誌著AI代理互操作性標準化的重大突破。這將為未來AI代理生態系統的開放、互通和健康發展奠定基礎，避免供應商鎖定，並促進跨平台、跨代理的協作。

*   **小型語言模型（SLM）在代理式AI中的潛力：**
    *   NVIDIA和Georgia Tech研究人員提出的「小型語言模型是代理式AI的未來」的觀點，指出了SLM在代理系統中的獨特優勢：足夠強大、更適合、更經濟。這項洞察將推動針對特定任務和效率需求，開發和部署更輕量、更高效的AI模型，實現更廣泛的邊緣AI和嵌入式AI代理應用。

*   **AI原生計算技術堆疊的整合與成熟：**
    *   圍繞PyTorch、ONNX、Transformers、DeepSpeed、Ray等形成的新訓練堆疊，以及以vLLM為事實標準、LLM-d為潛在核心的推論堆疊，正在快速成熟。這些開源項目的協同發展，使得大規模AI模型訓練和推論的效率和可擴展性得以顯著提升，為AI原生應用的普及提供了堅實的技術底座。

---

### 5. 趨勢洞察和未來展望

本次演講不僅是對當前AI技術的梳理，更是對未來AI發展趨勢的深刻洞察和預測，為業界勾勒出AI原生世界的宏偉藍圖。

**5.1 計算範式的演進：從雲到AI原生**
演講透過「歷史重演」的類比，明確指出計算領域正經歷從「雲計算」（VMs + OpenStack）到「雲原生計算」（Containers + Kubernetes），再到「AI原生計算」（Tokens + AI-Native Stack）的演進。這是一個質的飛躍，意味著：
*   **核心單位的轉變：** 從傳統的虛擬機和容器，轉變為以「tokens」為核心的AI模型交互單位。
*   **基礎設施的重塑：** 未來的基礎設施將不再僅僅優化數據儲存或容器編排，而是專為運行持續學習系統、處理tokens串流和代理工作流程、以及大規模優化推論而設計。這將帶來對硬體、軟體和網絡架構的全新要求。
*   **軟體開發的革命：** AI將「自行編寫軟體」，人類開發者的角色將從編寫程式碼轉向更高層次的「規劃、審查和協調」。這將徹底顛覆傳統軟體工程範式。

**5.2 開源與開放生態的必然趨勢**
演講者強烈主張「開源站在歷史的正確一邊」，並指出閉源的「護城河」是短命的。這一判斷基於以下事實和預期：
*   **性能追趕與超越：** 開源AI模型已證明其在性能上可與閉源模型抗衡甚至超越。
*   **加速創新與採用：** 開源模式透過開放權重、開放數據集和社群協作，極大地加速了技術迭代和市場採用，形成了強大的飛輪效應。
*   **避免供應商鎖定：** 透過開放協議和標準（如A2A納入Linux基金會），確保AI生態系統的互操作性，避免單一供應商的壟斷，保障企業和開發者的自由選擇權。
*   **集體投資與社群力量：** 構建開放平台需要巨大的集體投資，過去OpenStack和Kubernetes等開放雲平台獲得了數億美元的投入，而AI原生時代預計需要接近10億美元的投資。社群被視為實現這一目標的「重心」，透過「四開放原則」（開源、開放設計、開放開發、開放社群）來確保生態系統的健康發展。

**5.3 AI代理：未來軟體的形態**
AI代理被視為未來軟體開發和應用的核心趨勢：
*   **軟體焦點轉移：** 從「使用者速度」轉向「代理速度」，意味著軟體將為AI代理的無限吞吐量和高效率設計，而非僅限於人類輸入的速度。
*   **情境工程的重要性：** 這項新興技能將成為設計和優化AI代理的關鍵，影響代理的效率、成本和可靠性。
*   **小型語言模型的戰略地位：** SLM因其輕量、高效和成本效益，將在代理式AI中扮演越來越重要的角色，尤其在邊緣計算和特定領域應用中。

**5.4 基礎設施的持續挑戰與機遇**
儘管AI發展迅猛，基礎設施瓶頸仍是核心挑戰。這也意味著巨大的投資和創新機遇：
*   **多加速器調度：** 隨著不同硬體加速器（GPU、TPU、FPGA等）的發展，如何高效調度這些異構資源將是關鍵。
*   **效率為王：** 對於推論工作負載，追求極致的效率是永恆的主題，將驅動更多優化、快取和分散式技術的突破。
*   **整合的挑戰與社群的需求：** 簡報承認當前AI技術棧整合難度高，呼籲一個強大的社群來協調這些新興技術，形成統一、易用的平台。

**5.5 展望與呼籲**
演講最終發出強烈呼籲：「讓我們共同建立開放的AI未來！」這不僅是技術上的願景，更是一種協作精神的號召。預計未來將有更多像AI_DEV峰會這樣的平台，推動業界在開放治理、標準制定和社群建設方面加大投入，共同塑造AI原生時代的樣貌。確保AI原生時代是開放的，將是避免重蹈雲計算時代「閉源鎖定」覆轍的關鍵。

---

### 總結

Mark Collier先生的演講清晰地勾勒出AI原生世界的宏大圖景。它不僅定義了訓練、推論和代理這三大核心支柱，更深刻地闡釋了開源、基礎設施與社群在這一轉型中的決定性作用。AI的爆發式增長對現有基礎設施提出了嚴峻考驗，而開源模式的勝利已成為不可逆轉的趨勢，它不僅加速了創新，也為市場帶來顛覆性的變革。LLM作為新作業系統的思考，以及AI代理標準協議的成熟，都預示著軟體範式將發生根本性轉變。

展望未來，計算的核心將從應用程式轉向AI模型，從人類交互轉向代理協作。這需要業界巨大的集體投資，更需要一個強大、開放的社群作為「重心」，協調技術發展、制定標準、建立信任。正如Linux和Kubernetes開啟了開放的雲時代，開放的社群也將是確保AI原生時代真正開放、創新和繁榮的唯一途徑。這場變革既帶來挑戰，更帶來了前所未有的機遇，一個由開放、智能和協作驅動的未來正加速到來。

---

<div style="text-align: center; color: #666; font-size: 0.9em; margin-top: 2em;">
<em>本報告由 NeoTrendHub 自動生成 | 生成時間：2025-07-18 14:11:29</em>
</div>
