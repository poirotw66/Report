# SGLang 推理引擎——高效的开源部署方案

## 會議資訊
- **研討會：** 202506 AICon Beijing
- **類型：** 主題演講
- **來源：** [https://aicon.infoq.cn/2025/beijing/presentation/6453](https://aicon.infoq.cn/2025/beijing/presentation/6453)

---

## 報告內容

## SGLang 推理引擎：高效開源部署方案綜合分析報告

### 執行摘要

本報告旨在深入分析在 AiCon 2025 全球人工智能開發與應用大會上，由 LMSYS / SGLang 社區核心開發者尹良升發表的「SGLang 推理引擎——高效的開源部署方案」主題演講內容。SGLang 作為一個為大型語言模型（LLMs）和視覺語言模型（VLMs）提供服務的快速開源引擎，其核心價值在於解決當前 LLM 推理服務中面臨的效率、擴展性和成本挑戰。

報告將從會議概述、技術要點、商業價值、創新亮點以及趨勢洞察五個維度，全面剖析 SGLang 如何透過一系列前瞻性技術，如推測解碼、前綴-解碼解耦、專家並行負載均衡器及分層快取設計，實現業界領先的性能和可擴展性。SGLang 不僅在性能基準測試中表現卓越，更獲得了包括 XAI、NVIDIA、AMD 等眾多產業巨頭和學術機構的廣泛採用，確立了其在開源 LLM 服務領域的領導地位。

### 1. 會議概述與核心內容

本次演講的標題為「SGLang 推理引擎——高效的開源部署方案」，在 2025 年 6 月的 AiCon Beijing 大會上作為主題演講呈現。演講者尹良升作為 LMSYS / SGLang 社區的核心開發者，為聽眾帶來了關於 SGLang 這一「高效的開源大規模 LLM 服務框架」的深度解析。

**核心內容概述：**
簡報內容圍繞 SGLang 的核心功能、技術突破和生態系統展開，旨在展示 SGLang 如何在不斷發展的大型模型（LLMs & VLMs）部署中實現最先進的性能和效率。演講清晰地劃分為六個主要部分：

1.  **SGLang 里程碑與功能概述：** 介紹專案的發展歷程和關鍵功能，特別強調其在 Llama3、DeepSeek V3/R1 等模型上的領先地位。
2.  **SGLang 中的推測解碼與約束解碼：** 深入探討 SGLang 如何整合尖端解碼技術，顯著提升生成速度和輸出格式的精準控制。
3.  **PD 解耦的有效設計與實現：** 揭示 SGLang 為解決大規模服務中的前綴與解碼衝突問題而設計的創新架構。
4.  **大規模 EP 對 DeepSeek 部落格重現的支援：** 展示 SGLang 在專家並行（EP）方面的能力，尤其是在處理稀疏型模型如 MoE 上的優化。
5.  **SGLang 中的分層快取設計：** 闡述其高效記憶體管理策略，如何透過多層次快取機制提升資源利用率。
6.  **SGLang 的生態系統：** 強調其廣泛的社區支持和產業採用情況。

整場演講圍繞「高效」和「開源」兩大主軸，旨在為部署大規模 LLM 模型的開發者和企業提供一個強大且可行的解決方案。

### 2. 技術要點與實現細節

SGLang 的卓越性能源於其對 LLM 推理生命週期的全面優化，涵蓋了從記憶體管理、排程、解碼到並行計算的各個環節。

1.  **高效的 KV 快取管理：RadixAttention 與前綴快取**
    *   **技術原理：** SGLang 採用 **RadixAttention** 來處理複雜的鍵值（KV）快取重用模式。它能夠高效地執行前綴匹配、新數據插入和淘汰（eviction）操作。這種樹狀結構的 KV 快取重用，允許 SGLang 處理包含數十萬個令牌的複雜序列，極大地減少了重複計算，提升了記憶體效率。
    *   **實現細節：** 透過優化的資料結構和演算法，RadixAttention 確保了在動態請求環境下 KV 快取的最大化利用，這對於多輪對話或基於提示的應用至關重要。

2.  **零開銷的排程與執行：重疊排程器**
    *   **技術原理：** 傳統排程器在 CPU 和 GPU 之間存在同步阻塞，導致 CPU 在模型工作期間閒置，產生開銷。SGLang 引入了**重疊排程器 (Overlap Scheduler)**，實現了 CPU 和 GPU 任務的並行執行。
    *   **實現細節：** 透過協同排程（Cooperative Scheduling），利用「操作列表 (Operation List)」和「讓出點 (Yield Points)」來抽象化執行流程，使得排程器和模型工作者能夠在時間上重疊，從而消除了 CPU 開銷。Profiler 數據證實，這種設計顯著減少了 CPU 的等待時間（`sem_wait`）。

3.  **尖端解碼技術整合：推測解碼與約束解碼**
    *   **推測解碼 (Speculative Decoding)：**
        *   **技術原理：** SGLang 支援 **Eagle**（一種 SOTA 推測解碼演算法）和 **DeepSeek V3 的多令牌預測 (MTP)**。這些技術利用一個小型、更快的模型生成多個「草稿」令牌，然後由大型模型一次性驗證，大幅減少了大型模型的生成步數。
        *   **實現細節：** SGLang 是第一個與 EAGLE 團隊合作支援 EAGLE-3 的框架，在 Llama 3.1 8B 模型上，Eagle-3 實現了 2.4 倍的解碼速度提升。對於 DeepSeek-V3，MTP 方案在 H200 TP8 環境下也能帶來 1.5 到 1.8 倍的解碼速度提升，無論是單批次還是大批次場景。
    *   **約束解碼 (Constrained Decoding) 與 XGrammar：**
        *   **技術原理：** 為確保模型生成符合特定語法或格式的輸出（如 JSON），SGLang 與 **XGrammar** 實現了「零開銷整合」。這意味著約束條件的檢查和應用不會引入額外的性能瓶頸。
        *   **實現細節：** 透過優化 CPU/GPU 執行流程，避免了阻塞式的 Apply Mask 操作。在 JSON 解碼任務中，SGLang 比其他開源解決方案快高達 10 倍，極大提升了結構化數據生成的效率和準確性。

4.  **大規模部署優化：前綴-解碼解耦 (PD Disaggregation)**
    *   **問題背景：** 在大規模 LLM 服務中，前綴（Prefill，處理輸入提示）和解碼（Decode，生成輸出令牌）任務的混合會導致前綴中斷、資料並行注意力（DP Attention）負載不平衡，並與 DeepEP 等專家並行框架不相容。
    *   **解決方案：** SGLang 引入了革命性的 **PD 解耦架構**。
        *   **實現細節：** 該架構設計了獨立的**前綴實例 (Prefill Instances)** 和**解碼實例 (Decode Instances)**，並透過一個**統一負載均衡器 (Unified Load Balancer)** 進行請求路由。關鍵是支援**非阻塞 (non-blocking) 和基於 RDMA (Remote Direct Memory Access) 的 KV 傳輸**，確保前綴完成後，其 KV 快取能快速高效地傳輸到解碼實例，從而實現了兩個階段的真正解耦與並行。這解決了 DeepEP 調度模式與 DP Attention 的相容性問題。

5.  **高效並行策略：專家並行 (EP) 與兩批次重疊 (TBO)**
    *   **專家並行 (EP) for MoE：**
        *   **技術原理：** 針對稀疏型前饋網路（Sparse FFN），特別是專家混合模型（MoE），SGLang 透過專家並行在設備間劃分專家權重，從而擴展模型容量並優化記憶體。
        *   **實現細節：** 結合 DeepEP，它遵循「調度 → 專家 → 合併」模式，並透過**兩批次重疊 (Two-Batch-Overlap, TBO)** 技術最小化延遲和通訊開銷。
    *   **兩批次重疊 (TBO) 優化：**
        *   **問題背景：** 不當的啟動順序會導致 CPU 阻塞和 GPU 閒置。
        *   **解決方案：** SGLang 採用「計算 → 通訊」的**正確啟動順序**，確保在 CPU 阻塞的通訊任務（如 DeepEP 的 Dispatch）啟動之前，先將計算任務提交給 GPU，使得 GPU 在通訊過程中仍能保持活躍。
        *   **實現細節：** 透過簡潔的程式碼實現，利用協同排程的抽象化，有效地管理層邊界處的部分完成。
    *   **專家並行負載均衡器 (EPLB)：**
        *   **問題背景：** MoE 模型在擴展時容易出現負載不平衡，導致 GPU 閒置。
        *   **解決方案：** SGLang 引入 EPLB，透過更大的批次大小（減少路由隨機性）和**週期性重新平衡**策略來改善負載平衡性。
        *   **實現細節：** SGLang 透過 `torch P2P operations` 交換專家權重來實現週期性重新平衡，實驗證明 EPLB 顯著改善了大規模部署下的負載平衡。

6.  **分層快取設計 (Hierarchical Caching Design)：**
    *   **技術原理：** SGLang 採用多層次的快取架構，利用 GPU HBM、CPU DRAM 和外部儲存，以實現最大化的記憶體利用率和延遲隱藏。
    *   **實現細節：** 包含請求佇列、排程器、GPU 執行器、快取控制器和 HiRadixTree 等組件。其靈活的控制平面實現了資源感知型排程，高效的 I/O 資料平面則使用了專用的 CUDA 核心。SGLang 還提供與 Mooncake 等流行儲存層相容的通用 API，確保了與現有基礎設施的無縫整合。

### 3. 商業價值與應用場景

SGLang 的技術創新直接轉化為顯著的商業價值，使其成為大型模型部署領域的關鍵參與者。

1.  **顯著的成本效益：**
    *   **降低 GPU 資源消耗：** 憑藉領先的吞吐量（例如，在 96 個 GPU 上比傳統 TP 快 5 倍，比 DeepSeek MLA 快 7 倍），SGLang 允許企業在相同的硬體配置下處理更多的請求，或以更少的 GPU 資源滿足現有需求，從而大幅降低雲端計算或自建資料中心的營運成本。
    *   **優化能源效率：** 更高的硬體利用率和更少的閒置時間，意味著每生成一個令牌的能耗更低，符合綠色計算趨勢。

2.  **提升服務品質與用戶體驗：**
    *   **更低的延遲：** 推測解碼（EAGLE-3 提升 2.4 倍解碼速度）和重疊排程器消除了 CPU 開銷，使得模型響應速度更快，對於即時互動應用（如聊天機器人、AI 助手）至關重要。
    *   **更高的吞吐量與併發性：** 優化的批次處理、PD 解耦和 EPLB 確保了系統在高負載下仍能保持穩定和高效，支持更多的併發用戶和請求。

3.  **加速產品上市與創新：**
    *   **快速適配最新模型：** SGLang 能夠在 DeepSeek V3/R1 等新模型發佈後「首日支援」其專家並行和 PD 解耦，展示了其快速適應新技術的能力，使企業能迅速將最新的 AI 研究成果整合到產品中。
    *   **支援複雜應用場景：** 約束解碼（JSON 解碼快 10 倍）使得模型能夠生成精確、結構化的輸出，這對於需要特定格式響應的應用（如代碼生成、資料提取、RAG 系統中的精確答案）具有極高的商業價值。

4.  **解決大規模部署挑戰：**
    *   **可擴展性與穩定性：** PD 解耦架構從根本上解決了前綴與解碼的資源衝突和負載不平衡問題，為大規模、多節點 LLM 服務提供了穩定的基礎。專家並行負載均衡器（EPLB）確保了 MoE 模型在擴展時的效率和可靠性。
    *   **技術壁壘降低：** 作為開源框架，SGLang 降低了企業部署高性能 LLM 服務的技術門檻，使其能夠在不投入大量研發的情況下，獲得產業頂尖的推理能力。

**主要應用場景：**

*   **大型雲端服務提供商：** 用於構建高效、低成本的 LLM API 服務。
*   **企業級 AI 應用：** 部署內部 LLM 服務，用於智慧客服、知識管理、內容自動生成、程式碼輔助等。
*   **AI 基礎設施供應商：** 為 AI 硬體（如 NVIDIA、AMD GPU）提供高效的軟體堆疊。
*   **生成式 AI 新創公司：** 快速實現高併發、低延遲的模型推理服務。
*   **研究機構與 AI 開發者：** 作為高效的實驗和部署工具，尤其是在線上策略強化學習 (on-policy RLHF) 中，推理引擎對於高效的策略模型執行至關重要。

### 4. 創新亮點與技術突破

SGLang 的創新性體現在多個維度，不僅解決了現有 LLM 服務的痛點，更引領了未來高性能推理框架的發展方向。

1.  **開源領域的性能巔峰：** SGLang 實現了「所有開源 LLM 推理引擎中的最先進（SOTA）性能」，並「第一個在大型規模上幾乎達到 DeepSeek 官方部落格所報告吞吐量」的開源實現。其在 96 個 GPU 上比傳統張量並行快 5 倍，比 DeepSeek MLA 快 7 倍的數據，是其性能突破的直接證明。這不僅僅是優化，更是系統性創新帶來的質變。

2.  **開創性的前綴-解碼解耦 (PD Disaggregation) 架構：** 這是 SGLang 最具標誌性的創新之一。通過將 LLM 推理的兩個核心階段（前綴和解碼）從架構層面徹底分離，並實現高效的 KV 數據傳輸，SGLang 根本性地解決了傳統推理引擎中前綴請求對解碼的干擾、資源不平衡以及與專家並行模式不兼容等長期存在的問題。這是一種對 LLM 服務架構的**重新思考和創新**。

3.  **領先的推測解碼整合與創新：** SGLang 不僅支援最先進的 Eagle 推測解碼演算法（並作為「第一個與 EAGLE 團隊合作支援 EAGLE-3 的框架」），還為 DeepSeek V3 實現了獨特的 MTP 推測解碼。這表明 SGLang 能夠迅速吸收並推進最新的學術研究成果，將其轉化為實際部署中的巨大性能提升。

4.  **智慧且高效的資源管理與排程：**
    *   **零開銷排程器：** 透過 CPU 和 GPU 任務的精妙重疊，SGLang 徹底消除了傳統排程器的 CPU 閒置問題，最大化了硬體利用率。這種協同排程的實現方式本身就是一個工程創舉。
    *   **專家並行負載均衡器 (EPLB)：** 針對 MoE 模型在擴展時的負載不平衡這一複雜難題，SGLang 提供了有效的解決方案。透過動態調整和週期性重新平衡，確保了大規模 MoE 部署的效率，這是稀疏模型推理領域的一大突破。
    *   **分層快取設計與 HiRadixTree：** 這種多層次、資源感知型的快取體系，結合專用的 `CUDA kernels` 和靈活的控制平面，極大地優化了 KV 快取的管理和 I/O 效率，有效隱藏了延遲。

5.  **「首日支援」與快速生態適應：** SGLang 在 DeepSeek V3/R1 模型發佈後迅速提供了專家並行和 PD 解耦的開源支援，顯示了其核心開發團隊對前沿模型和技術的深刻理解以及快速實現能力。這使得 SGLang 成為新模型部署的「引領者」而非「追隨者」。

6.  **簡潔且高效的軟體工程實踐：** 兩批次重疊（TBO）的「簡潔實現」，透過操作列表和讓出點的抽象，以及避免不當啟動順序，體現了卓越的軟體工程能力。這種設計不僅提升了性能，也確保了程式碼的可維護性和可擴展性。

這些創新亮點共同構成了 SGLang 作為 LLM 推理服務框架的獨特競爭力，使其在性能、擴展性、靈活性和成本效益方面均達到業界領先水平。

### 5. 趨勢洞察與未來展望

SGLang 的發展和成功，不僅反映了當前 LLM 服務的關鍵趨勢，也預示了未來 AI 基礎設施演進的方向。

1.  **超大規模模型成為主流，對高效推理框架需求激增：** 隨著 DeepSeek V3、Llama 3.1 等百億甚至萬億參數級模型的湧現，尤其是 MoE 等稀疏模型的普及，對推理引擎的效率和可擴展性提出了前所未有的要求。SGLang 透過 EP、PD 解耦等技術，直接響應了這一趨勢，成為支撐這些超大模型商業化應用的關鍵。

2.  **成本優化是 LLM 部署的永恆主題：** 大型模型的推理成本高昂，是企業應用 AI 的主要障礙之一。SGLang 從吞吐量、硬體利用率、記憶體管理等多維度進行優化，旨在降低每令牌的服務成本，這與市場對「更經濟、更可負擔的 AI」的普遍需求高度契合。未來，對更低能耗、更高性價比的推理方案的追求將持續推動技術進步。

3.  **多模態與具身智能的興起對服務框架提出新挑戰：** 簡報中提到 SGLang 支援 VLM（視覺語言模型），並且 AiCon 會議規劃中涵蓋了「多模態大模型」和「具身智能」等主題。這表明未來的 AI 應用將不僅限於文本，還會整合視覺、聽覺、動作等多元模態。推理框架需要具備處理多模態輸入和輸出、更複雜的上下文以及即時交互的能力。SGLang 的靈活架構和高性能設計為此奠定了基礎。

4.  **硬體與軟體棧的深度協同：** SGLang 與 NVIDIA、AMD 等硬體廠商的緊密合作，以及對特定硬體特性（如 RDMA、CUDA kernels）的利用，預示著未來高性能 AI 基礎設施將更加強調軟硬體一體化的設計與優化。純軟體層面的優化將逐漸觸及性能瓶頸，而深度整合硬體加速能力將成為提升性能的關鍵。

5.  **開源生態系統的影響力日益增強：** SGLang 作為 LMSYS Org 孵化的開源專案，擁有 400 多名貢獻者和廣泛的產業採用。這證明了開源模式在匯聚智慧、加速創新和推動技術普及方面的巨大潛力。未來，更多核心 AI 技術將在開放協作的環境中誕生和成熟。

6.  **精準控制和結構化生成的重要性：** SGLang 對約束解碼（XGrammar）的支援，滿足了企業應用中對模型輸出精度和格式的嚴格要求。這預示著通用大模型將逐漸演變為能夠生成特定格式、滿足特定業務邏輯的「智能部件」，使得 LLM 更容易被整合到現有業務流程中，從而釋放更大的商業價值。

**未來展望：**

SGLang 在 LLM 推理引擎領域的領先地位，使其有望在未來繼續引領技術潮流。展望未來，SGLang 可能會：
*   **深化對異構硬體的支援：** 隨著更多 AI 晶片和加速器的出現，SGLang 將進一步優化其在不同硬體平台上的性能表現。
*   **探索更複雜的服務模式：** 支援更動態、自適應的批次處理和調度策略，以應對更不可預測的生產環境負載。
*   **整合更多模型優化技術：** 例如更先進的量化、稀疏化技術，以進一步降低模型部署所需的資源。
*   **拓展多模態和具身智能應用：** 為 VLM 和未來智能體的即時、高效交互提供更強大的支持。
*   **持續推動社區協作：** 吸引更多貢獻者和使用者，形成更為健壯和多元的生態系統，共同應對 AI 發展的挑戰。

總之，SGLang 不僅是當前 LLM 服務領域的技術集大成者，更是引領未來 AI 基礎設施發展的重要力量。其不斷的技術創新和廣泛的產業影響力，將持續推動 AI 應用的普及和深化。

### 結論

SGLang 是一個為大型語言模型和視覺語言模型量身打造的**高性能、開源且高度優化**的服務框架。本次 AiCon 演講全面展示了其在解決 LLM 推理效率和可擴展性方面所做的卓越貢獻。

SGLang 的核心競爭力體現在以下幾個方面：
*   **極致的性能和效率：** 透過 RadixAttention、零開銷排程器、推測解碼（Eagle, MTP）和約束解碼（XGrammar）等技術，SGLang 在多項基準測試中實現了業界領先的吞吐量和速度，顯著降低了推理成本。
*   **革命性的架構創新：** 前綴-解碼解耦（PD Disaggregation）是其最突出的突破，有效解決了大規模 LLM 服務中的關鍵瓶頸，並確保了與專家並行（EP）等先進並行策略的無縫協同。
*   **強大的可擴展性：** 針對 MoE 模型設計的專家並行負載均衡器（EPLB）和兩批次重疊（TBO），確保了模型在數十甚至數百個 GPU 上仍能維持高效運行和負載平衡。
*   **前瞻性的設計：** 分層快取設計、對新模型的快速支援（如 DeepSeek V3/R1），以及對多模態的適應性，都使其能夠跟上 AI 技術的快速發展。
*   **堅實的開源生態：** 作為 LMSYS Org 孵化的專案，SGLang 擁有活躍的貢獻者社群和廣泛的產業採用，證明了其在實際應用中的價值和可靠性。

SGLang 不僅是一個技術先進的框架，更是一個賦能 AI 應用普及和創新的關鍵基礎設施。它在軟體工程上的精妙設計和對硬體潛力的最大化挖掘，為開發者和企業提供了一個可靠、高效、可擴展的解決方案，使其能夠更輕鬆、更經濟地將大型語言模型部署到生產環境中，共同探索 AI 應用的無限邊界。

---

<div style="text-align: center; color: #666; font-size: 0.9em; margin-top: 2em;">
<em>本報告由 NeoTrendHub 自動生成 | 生成時間：2025-07-18 14:11:33</em>
</div>
